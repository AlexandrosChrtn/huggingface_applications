{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85aa69a7-564b-467c-98d2-c3f3d30b649c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandroschariton/virtual_environments_py/pythess_demo_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4808491e-62c0-4eea-ada6-1e698beeba21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 200\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the base model and tokenizer\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the training dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"sarcasm_data/sarcasm.csv\", split=\"train\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0227454-5ab1-44e2-8b77-e3af2aaae59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What year is it?',\n",
       " 'answer': \"Oh, weâ€™re still in 2024, believe it or not. Time flies when you're... asking questions like this.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f32208f-0901-40c6-bfca-b5ff2765d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the chat template\n",
    "def apply_chat_template(example):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example['question']},\n",
    "        {\"role\": \"assistant\", \"content\": example['answer']}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return {\"prompt\": prompt}\n",
    "\n",
    "# Apply the function to the dataset\n",
    "new_dataset = dataset.map(apply_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "224e662b-6ad8-4184-89ce-9864317dbf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'prompt'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11bd1f6c-1331-49bf-bc41-854e4a1bfb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What year is it?',\n",
       " 'answer': \"Oh, weâ€™re still in 2024, believe it or not. Time flies when you're... asking questions like this.\",\n",
       " 'prompt': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 11 Oct 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat year is it?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nOh, weâ€™re still in 2024, believe it or not. Time flies when you're... asking questions like this.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11c33938-b173-4539-be06-fcad10507ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = new_dataset.train_test_split(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14bbb58c-27fa-4f26-9db1-5b01d3f4131d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'prompt'],\n",
       "        num_rows: 190\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'prompt'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cbdd3d3-3c57-4d5e-84dc-46cea91ca36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190/190 [00:00<00:00, 5854.31 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 2490.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'prompt', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 190\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'prompt', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the data\n",
    "def tokenize_function(example):\n",
    "    tokens = tokenizer(example['prompt'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    # Set padding token labels to -100 to ignore them in loss calculation\n",
    "    tokens['labels'] = [\n",
    "        -100 if token == tokenizer.pad_token_id else token for token in tokens['input_ids']\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = new_dataset.map(tokenize_function)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b860e936-9df9-4681-97ae-238101620729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 190\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns(['question', 'answer', 'prompt'])\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fbe3f8d-6857-4a18-adc6-be989ef30a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandroschariton/virtual_environments_py/pythess_demo_venv/lib/python3.12/site-packages/transformers/training_args.py:2199: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 190\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 190\n",
      "  Number of trainable parameters = 1,235,814,400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [190/190 03:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.506400</td>\n",
       "      <td>1.376111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.120200</td>\n",
       "      <td>0.990449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.575200</td>\n",
       "      <td>1.016172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.375800</td>\n",
       "      <td>0.979048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./results/checkpoint-150\n",
      "Configuration saved in ./results/checkpoint-150/config.json\n",
      "Configuration saved in ./results/checkpoint-150/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-150/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-150/special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./results/checkpoint-190\n",
      "Configuration saved in ./results/checkpoint-190/config.json\n",
      "Configuration saved in ./results/checkpoint-190/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-190/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-190/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-190/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./fine-tuned-model\n",
      "Configuration saved in ./fine-tuned-model/config.json\n",
      "Configuration saved in ./fine-tuned-model/generation_config.json\n",
      "Model weights saved in ./fine-tuned-model/model.safetensors\n",
      "tokenizer config file saved in ./fine-tuned-model/tokenizer_config.json\n",
      "Special tokens file saved in ./fine-tuned-model/special_tokens_map.json\n",
      "tokenizer config file saved in ./fine-tuned-model/tokenizer_config.json\n",
      "Special tokens file saved in ./fine-tuned-model/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-model/tokenizer_config.json',\n",
       " './fine-tuned-model/special_tokens_map.json',\n",
       " './fine-tuned-model/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "model.train()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",  # to evaluate during training\n",
    "    eval_steps=40,\n",
    "    logging_steps=40,\n",
    "    save_steps=150,\n",
    "    per_device_train_batch_size=2,  # Adjust based on your hardware\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,  # Modify based on your dataset size\n",
    "    fp16=False,  # Enable mixed precision if available\n",
    "    save_total_limit=2,  # Only save the last two checkpoints\n",
    "    report_to=\"tensorboard\",\n",
    "    log_level=\"info\",\n",
    "    learning_rate=1e-5,\n",
    "    use_mps_device=True,\n",
    "    max_grad_norm=2\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "trainer.save_model(\"./fine-tuned-model_1b\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-model_1b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
